\section{Schema Management}

Here we discuss control of the schema.
How do we manage changes in a controlled manner which do not break our systems?
Can we do that and still remain flexible enough for a large organisation to develop with relative ease?

The DPDD is a project-level change-controlled document it shall remain subject to the RFC and DM CCB  procedures for modifying it.

The ``specification'' schema in LDM-153 should be updated whenever necessary to reflect any changes
in DM's planned data products at the end of construction. Evolution of the LDM-153 schema is
expected during construction as the pipelines evolve and the measurement outputs are better
understood.

LDM-153 is change-controlled by the DM-CCB, and its contents are generated from the baselineSchema.yaml file
in the \texttt{sdm\_schemas} repository. Procedurally, \texttt{baselineSchema.yaml} should be
treated like any other LaTeX input for a change controlled document: changes to it may be merged to
master via a normal ticket, but are not ``official'' until an approved RFC releases a new version of
the LDM document.  This allows some flexibility in development while maintaining control of the official version.

The \texttt{hsc.yaml} schema in \texttt{sdm\_schemas} is a ``concrete'' schema that is not subject
to change control, but the \texttt{ci\_hsc} integration tests verify that the outputs from that
pipeline execution comply with the schema specified in \texttt{hsc.yaml}. This ensures that an
up-to-date schema is always available, so that steps like loading an HSC reprocessing run into qserv
do not require fixing up all the changes to the schema since the previous ingest.

All other schemas in the \texttt{sdm\_schemas} repository are ``concrete'' schemas reflecting
specific sets of data products; these may be edited as necessary by a normal ticket workflow.

%\textbf{TODO:} we only have one hsc.yaml, should we be creating more reprocessing-specific copies?
%I.e. one for each RC2 reprocessing and saving them in separate files in sdm\_schemas.

\subsection{Science Data Model and Science Pipelines}
Data products that are generated by the science pipelines must conform to a physical Felis-defined
schema in order to be loaded into databases. The pipelines generate a variety of intermediate
catalog products, often in afwTable FITS files, which must be transformed into the user-facing
tables by the pipelines team. This transformation is more than naming and units: e.g. fluxes may
need to have calibrations applied, or uncertainties in pixel units may be transformed to angular
units by using WCS information.

This transformation process is not change controlled; the pipelines team has full control over how
the user-facing tables are generated. It is only the definition of the user-facing tables that is
change-controlled.

The pipelines code implements the various column transformations via a series of ``functors'', each
dedicated to a particular type of transformation, and the definition of which functors are applied
to which columns is defined by a YAML configuration file. This YAML file has an entirely distinct
format from the Felis-defined schemas, and its purpose is distinct. The pipelines code generates
data products that comply with a particular Felis schema, but there is no automatic linkage between
the two. Simple verification can be performed on the pipelines' YAML file to ensure that it
generates columns with the correct names, but any changes to the output data products require both
the Felis YAML and the pipelines' YAML to have corresponding updates applied.


